{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9773c13",
   "metadata": {},
   "source": [
    "# Reading and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77115596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import timedelta  # Not actually used, but shows a nice Ploomber feature\n",
    "from random import choice\n",
    "\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch as t\n",
    "import neuralforecast as nf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c482a8",
   "metadata": {},
   "source": [
    "## Read csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad8d219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data can be downloaded here: https://ourworldindata.org/covid-cases\n",
    "df = pd.read_csv(\"~/Downloads/owid-covid-data.csv\", parse_dates=['date'])\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afeac8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to be used in the model as extra information\n",
    "X_cols = [\"human_development_index\", \"gdp_per_capita\", \"stringency_index\", \n",
    "          \"life_expectancy\", \"hospital_beds_per_thousand\"]\n",
    "# Column to predict\n",
    "Y_col =  \"new_cases\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d40209",
   "metadata": {},
   "source": [
    "## Cleaning and converting into desired format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273edd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These names are required by the library\n",
    "column_renamer = {\"location\": \"unique_id\",\n",
    "                  \"date\": \"ds\",\n",
    "                  \"new_cases\": \"y\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c4ddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library requires that all series end at the same time. Not many countries are lost this way\n",
    "df = df[df.groupby(\"location\")[\"date\"].transform(max) == df[\"date\"].max()]\n",
    "minmaxes = df.groupby(\"location\")[\"date\"].agg([min, max, \"count\"])\n",
    "minmaxes[\"proportions\"] = (((minmaxes[\"max\"] - minmaxes[\"min\"]).dt.days + 1) / minmaxes[\"count\"])\n",
    "minmaxes[minmaxes[\"proportions\"] != 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564f9663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating data to be at a weekly level since daily was very messy\n",
    "grouper_aggregator = {col: \"mean\" for col in X_cols}\n",
    "grouper_aggregator.update({Y_col: \"sum\"})\n",
    "df = df.groupby([\"location\", \n",
    "                 pd.Grouper(key='date', freq='W-MON')]).agg(grouper_aggregator).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632d4baa",
   "metadata": {},
   "source": [
    "## Creating DataFrame for Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e481ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_df = df[[\"location\", \"date\", Y_col]]\n",
    "Y_df = Y_df.rename(columns=column_renamer).fillna(0)\n",
    "Y_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c27818",
   "metadata": {},
   "source": [
    "## Creating DataFrame for X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcfe882",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = df[[\"location\", \"date\"] + X_cols]\n",
    "X_df = X_df.rename(columns=column_renamer).fillna(0)\n",
    "X_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ebae40",
   "metadata": {},
   "source": [
    "## Split Test and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f399ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = 6  # This is how many periods you wish predict. 6 weeks in this case\n",
    "Y_df_test = Y_df.groupby('unique_id').tail(output_size)\n",
    "Y_df_train = Y_df.drop(Y_df_test.index)\n",
    "\n",
    "X_df_train = X_df.drop(Y_df_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974f5435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since I want to show a comparable result along with my predictions,\n",
    "# output split is to be doubled so that I can have train  >  validation  >  predictions\n",
    "#                                                         (output_size1)   (output_size2)\n",
    "input_size = 2 * output_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae356c8",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34bf70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This library tool tags the data as train and validation. See graph below\n",
    "train_mask_df, val_mask_df, _ = nf.experiments.utils.get_mask_dfs(\n",
    "    Y_df=Y_df_train,\n",
    "    ds_in_val=output_size,\n",
    "    ds_in_test=0\n",
    ")\n",
    "train_mask_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085058b6",
   "metadata": {},
   "source": [
    "## Plot of train/test proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cae575a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = Y_df_train.merge(\n",
    "    train_mask_df.drop('available_mask', axis=1).rename(columns={'sample_mask': 'sample_mask_train'}),\n",
    "    how='left',\n",
    "    on=['unique_id', 'ds']\n",
    ").merge(\n",
    "    val_mask_df.drop('available_mask', axis=1).rename(columns={'sample_mask': 'sample_mask_val'}),\n",
    "    how='left',\n",
    "    on=['unique_id', 'ds']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa00989",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df['y_train'] = np.where(plot_df['sample_mask_train'] == 1, plot_df['y'], np.nan)\n",
    "plot_df['y_val'] = np.where(plot_df['sample_mask_val'] == 1, plot_df['y'], np.nan)\n",
    "plot_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d12047",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df.query('unique_id == \"Argentina\"').set_index('ds')[['y_train', 'y_val']].plot(title=\"Train vs Validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1257f7",
   "metadata": {},
   "source": [
    "## Converting into WindowsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cca401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an optimized window object made by Nixtla. It can handle time series more efficiently\n",
    "train_dataset = nf.data.tsdataset.WindowsDataset(\n",
    "    Y_df=Y_df_train, \n",
    "    X_df=X_df_train,\n",
    "     f_cols=X_cols,\n",
    "    input_size=input_size,\n",
    "    output_size=output_size,\n",
    "    mask_df=train_mask_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451a36c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an optimized window object made by Nixtla. It can handle time series more efficiently\n",
    "val_dataset = nf.data.tsdataset.WindowsDataset(\n",
    "    Y_df=Y_df_train, \n",
    "     X_df=X_df_train,\n",
    "     f_cols=X_cols,\n",
    "    input_size=input_size,\n",
    "    output_size=output_size,\n",
    "    mask_df=val_mask_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb78053",
   "metadata": {},
   "source": [
    "## Converting into TimeSeriesLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f6d3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is another object made by Nixtla. It's used to give the WindowsDatasets to the models\n",
    "train_loader = nf.data.tsloader.TimeSeriesLoader(\n",
    "    train_dataset, batch_size=32, \n",
    "    n_windows=256,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c59a2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is another object made by Nixtla. It's used to give the WindowsDatasets to the models\n",
    "val_loader = nf.data.tsloader.TimeSeriesLoader(\n",
    "    val_dataset, \n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca48d52",
   "metadata": {},
   "source": [
    "## Instancing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf491898",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nf.models.nbeats.nbeats.NBEATS(\n",
    "    n_time_in=input_size,\n",
    "    n_time_out=output_size,\n",
    "    n_x=len(X_cols),  # Number of X cols you provided\n",
    "    n_x_hidden=[len(X_cols), 10], # These are neural network hidden layers for X variables\n",
    "    frequency='W-MON', \n",
    "    seasonality=4  # Since they are weeks. It would be 7 in case of daily or any other number if you know what you are doing\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc894f7d",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4804e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "early_stopping = pl.callbacks.EarlyStopping(monitor=\"val_loss\")\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=50,  # The bigger this number, the deeper the model is\n",
    "                     gpus=-1 if t.cuda.is_available() else 0,\n",
    "                     callbacks=[early_stopping])\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd0923c",
   "metadata": {},
   "source": [
    "## Forecasting using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845317d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_df_forecast = model.forecast(Y_df_train, X_df=X_df)\n",
    "Y_df_forecast.rename(columns={'y': 'y_hat'}, inplace=True)\n",
    "Y_df_forecast.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7d726e",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161f789b",
   "metadata": {},
   "source": [
    "## Random country plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062068bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_df_plot = Y_df_test.append(Y_df_train).merge(Y_df_forecast, how='left', on=['unique_id', 'ds']).sort_values(\"ds\")\n",
    "country = choice(Y_df_plot[\"unique_id\"].unique())\n",
    "Y_df_plot.query(f'unique_id == \"{country}\"').tail(100).set_index('ds').plot(title=country)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
